{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to convert solar and wind mesh grid data to aggregate county data for modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import os\n",
    "import reverse_geocoder as rg\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "cdat = pd.read_csv('Texas_Counties.csv', usecols=['County']) #all 254 counties in TX\n",
    "counties = cdat['County'].to_list()\n",
    "\n",
    "non_ercot_counties = ['El Paso','Hudspeth','Gaines','Terry','Yoakum','Cochran','Hockley','Lubbock','Bailey',\n",
    "                      'Lamb','Hartley','Dallam','Moore','Sherman','Hansford','Hutchinson','Ochiltree','Lipscomb', \n",
    "                      'Hemphill','Bowie','Morris','Cass','Camp','Marion','Upshur','Gregg','Harrison','Panola',\n",
    "                      'Shelby','San Augustine','Sabine','Trinity','Polk','Tyler','Jasper','Newton','San Jacinto',\n",
    "                      'Hardin','Liberty','Orange','Jefferson']\n",
    "\n",
    "remove_counties = [count + ' County' for count in non_ercot_counties]\n",
    "\n",
    "for count in remove_counties:\n",
    "    counties.remove(count)\n",
    "\n",
    "print(len(counties))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "All the NREL Wind Toolkit and SAM Solar data has been stored in google drive for space considerations.\n",
    "Pydrive is being employed to access and work with this data - next 3 blocks are for doing this.\n",
    "Data is converted to 'Wind2009.csv' type files, which are used in modeling and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pydrive quickstart google authorization - if you would like to use google drive for data storage\n",
    "#consult pydrive quickstart instructions to set this up\n",
    "from pydrive.auth import GoogleAuth\n",
    "\n",
    "gauth = GoogleAuth()\n",
    "gauth.LocalWebserverAuth() # Creates local webserver and auto handles authentication.\n",
    "\n",
    "#pydrive drive interaction\n",
    "from pydrive.drive import GoogleDrive\n",
    "\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solar reformatting\n",
    "\n",
    "#get files from drive - put google drive folder id in parantheses\n",
    "file_list = drive.ListFile({'q': \"'1pqhBKFg6FGNOqTwyNs9dq5J2XChaWBUh' in parents and trashed=false\"}).GetList()\n",
    "\n",
    "#reformatting - want average capacity of all coordinates within county\n",
    "countydic = dict.fromkeys(counties,np.zeros(17520)) #every half hour\n",
    "countdic = dict.fromkeys(counties,0)\n",
    "wcn = 0 #how many points in our mesh grid are not within an ERCOT county\n",
    "\n",
    "for file in file_list:\n",
    "    name = file['title'] #files are titled by coordinate\n",
    "    lat = name.split('lat')[1]\n",
    "    [lat,lon] = lat.split('lon')\n",
    "    lon = lon.split('.csv')[0]\n",
    "    lon = lon.split(' ')[0]\n",
    "    coordinates = (float(lat),float(lon))\n",
    "    county = rg.search(coordinates)[0]['admin2'] #reverse_geocoder used to site coordinates in county\n",
    "    print(county)\n",
    "    if county in counties:\n",
    "        file = drive.CreateFile({'id': file['id']})\n",
    "        file.GetContentFile('file.csv')\n",
    "        pdat = pd.read_csv('file.csv', usecols=['PowerGen_kW'],dtype=np.float).to_numpy()\n",
    "        pdat = [float(i) for i in pdat]\n",
    "        countydic[county] = countydic[county] + pdat\n",
    "        countdic[county] += 1\n",
    "    else:\n",
    "        wcn += 1\n",
    "\n",
    "nameplate_cap = 22557.15 #nameplate capacity of used sample sites in kW\n",
    "outdic = dict.fromkeys(counties,np.zeros(17520))\n",
    "for county in counties:\n",
    "    if countdic[county] != 0:\n",
    "        #computing average solar capacity within each county\n",
    "        outdic[county] = countydic[county] / (countdic[county]*nameplate_cap)\n",
    "\n",
    "#df = pd.DataFrame(data=outdic).to_csv('Solar/Solar2011.csv')\n",
    "print(len(file_list),wcn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wind reformatting\n",
    "\n",
    "#get files from drive - put google drive folder id in parantheses\n",
    "file_list = drive.ListFile({'q': \"'1H40oHl84pa8iaJ7xY4UpUlY78kvMlc2l' in parents and trashed=false\"}).GetList()\n",
    "\n",
    "\n",
    "#reformatting - max site per county\n",
    "countydic = {}\n",
    "sumdic = dict.fromkeys(counties,0) #keeping track of best site so far (in terms of average capacity factor)\n",
    "#filedic = {} #keeping track of best location within each county - not necesary\n",
    "wcn = 0 #how many points in our mesh grid are not within an ERCOT county\n",
    "\n",
    "for file in file_list:\n",
    "    name = file['title']\n",
    "    lat = name.split('lat')[1]\n",
    "    [lat,lon] = lat.split('lon')\n",
    "    lon = lon.split('.csv')[0]\n",
    "    coordinates = (float(lat),float(lon))\n",
    "    county = rg.search(coordinates)[0]['admin2']\n",
    "    if county in counties:\n",
    "        file = drive.CreateFile({'id': file['id']})\n",
    "        file.GetContentFile('file.csv')\n",
    "        pdat = pd.read_csv('file.csv', skiprows=3, usecols=['Capacity Factor'],dtype=np.float, engine='python').to_numpy()\n",
    "        dat = []\n",
    "        ssum = 0\n",
    "        for hour in range(17520):\n",
    "            power = sum([float(i) for i in pdat[(6*hour):(6*(hour+1))]]) / 6 #5 min --> 30 min intervals\n",
    "            dat.append(power)\n",
    "            ssum += power\n",
    "        #for getting max site\n",
    "        if ssum > sumdic[county]: #is this site \"better\" than current best site in county\n",
    "            countydic[county] = dat\n",
    "            sumdic[county] = ssum\n",
    "            #filedic[county] = [lat,lon]\n",
    "    else:\n",
    "        wcn += 1\n",
    "\n",
    "#pd.DataFrame(data=countydic).to_csv('Wind/Wind2011.csv')\n",
    "print(wcn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Coal reformatting - import coal data and format to be by the half hour\n",
    "fuel = 'ERCOT/Fuel2019.xlsx'\n",
    "months = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\n",
    "xls = pd.ExcelFile(fuel)\n",
    "\n",
    "coalhours = np.empty(0)\n",
    "\n",
    "for i in range(12):\n",
    "    dat = pd.read_excel(xls, months[i])\n",
    "    coal = dat.loc[dat['Fuel']==\"Coal\"]\n",
    "    coal = coal[coal.columns[4::2]].to_numpy() + coal[coal.columns[5::2]].to_numpy() #every half hour\n",
    "    coal = np.reshape(coal,np.prod(coal.shape))\n",
    "    coalhours = np.concatenate((coalhours,coal))\n",
    "coalhours = np.nan_to_num(coalhours)\n",
    "\n",
    "pd.DataFrame(data={'Coal': coalhours}).to_csv('ERCOT/Coal2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REGIONS (for doing regional analysis)\n",
    "from uszipcode import SearchEngine\n",
    "search = SearchEngine(simple_zipcode=True)\n",
    "\n",
    "pdat = pd.read_excel('ERCOT/Zipcode_Data.xlsx', sheet_name='ZipToZone', usecols=['Svc. Address ZIP Code','Weather Zone Code'])\n",
    "\n",
    "#removing zipcodes outside of ERCOT\n",
    "update = pd.read_excel('ERCOT/Zipcode_Data_Update.xlsx', sheet_name='D1', skiprows=3, usecols=['Zipcode']).to_numpy().flatten()\n",
    "dropidx = pdat.index[pdat['Svc. Address ZIP Code'].isin(update)].to_list()\n",
    "\n",
    "pdat = pdat.drop(dropidx)\n",
    "\n",
    "regions = np.array(['NORTH', 'NCENT', 'EAST', 'COAST', 'SOUTH', 'SCENT', 'WEST', 'FWEST']) #easier if numpy array\n",
    "regdict = {'NORTH': [], 'NCENT': [], 'EAST': [], 'COAST': [], 'SOUTH': [], 'SCENT': [], 'WEST': [], 'FWEST': []}\n",
    "\n",
    "for i,row in pdat.iterrows():\n",
    "    zipcode = str(row['Svc. Address ZIP Code'])\n",
    "    region = str(row['Weather Zone Code'])\n",
    "    county = search.by_zipcode(zipcode).to_dict()['county']\n",
    "    regdict[region].append(county)\n",
    "    \n",
    "#some counties in multiple regions - could put multiple region counties in primary region or just leave in multiple regions\n",
    "#every county pretty dominantly in one county except austin (even split)\n",
    "simpregdict = {}\n",
    "multregdict = {}\n",
    "for county in counties:\n",
    "    counter = []\n",
    "    for reg in regions:\n",
    "        counter.append(regdict[reg].count(county))\n",
    "    simpregion = regions[np.argmax(counter)]\n",
    "    multregions = list(regions[[i for i, val in enumerate(counter) if val != 0]])\n",
    "    simpregdict[county] = [simpregion] #for simple can do county --> region cause only 1 region per county\n",
    "    multregdict[county] = multregions\n",
    "\n",
    "simpregdict['Austin County'] = 'SCENT' #austin between two zones put in SCENT b/c map looks like that\n",
    "pd.DataFrame(data=simpregdict).to_csv('CountyToRegion.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.925179186758031\n",
      "517.1751653977393\n"
     ]
    }
   ],
   "source": [
    "#import coal plant data and format to be regional\n",
    "pdat = pd.read_csv('EPA_Coal_2019.csv',usecols=['Facility_Name','Gross_Load_MW'])\n",
    "\n",
    "oklaunion = pdat.loc[pdat['Facility_Name']==\"Oklaunion Power Station\"]['Gross_Load_MW'].to_numpy()\n",
    "coletocreek = pdat.loc[pdat['Facility_Name']==\"Coleto Creek\"]['Gross_Load_MW'].to_numpy()\n",
    "martinlake = pdat.loc[pdat['Facility_Name']==\"Martin Lake\"]['Gross_Load_MW'].to_numpy()\n",
    "fayette = pdat.loc[pdat['Facility_Name']==\"Sam Seymour\"]['Gross_Load_MW'].to_numpy()\n",
    "sanmiguel = pdat.loc[pdat['Facility_Name']==\"San Miguel\"]['Gross_Load_MW'].to_numpy()\n",
    "sandycreek = pdat.loc[pdat['Facility_Name']==\"Sandy Creek Energy Station\"]['Gross_Load_MW'].to_numpy()\n",
    "oakgrove = pdat.loc[pdat['Facility_Name']==\"Oak Grove\"]['Gross_Load_MW'].to_numpy()\n",
    "jkspruce = pdat.loc[pdat['Facility_Name']==\"J K Spruce\"]['Gross_Load_MW'].to_numpy()\n",
    "limestone = pdat.loc[pdat['Facility_Name']==\"Limestone\"]['Gross_Load_MW'].to_numpy()\n",
    "waparish = pdat.loc[pdat['Facility_Name']==\"W A Parish\"]['Gross_Load_MW'].to_numpy()\n",
    "twinoaks = pdat.loc[pdat['Facility_Name']==\"Twin Oaks\"]['Gross_Load_MW'].to_numpy()\n",
    "\n",
    "#to convert gross output to net output, using top-level ratio of (net coal output in ERCOT / gross coal output in ERCOT)\n",
    "coalgross = coletocreek + sanmiguel + jkspruce + fayette + limestone + sandycreek + oklaunion + martinlake + oakgrove + twinoaks + waparish\n",
    "ratio = sum(coalhours) / sum(coalgross)\n",
    "print(ratio)\n",
    "\n",
    "#siting in a region based on county each plant is located within\n",
    "southcoal = ratio*(coletocreek + sanmiguel)\n",
    "scentcoal = ratio*(jkspruce + fayette)\n",
    "ncentcoal = ratio*(limestone + sandycreek)\n",
    "northcoal = ratio*(oklaunion)\n",
    "eastcoal = ratio*(martinlake + oakgrove + twinoaks)\n",
    "coastcoal = ratio*(waparish)\n",
    "westcoal = np.zeros(8760)\n",
    "fwestcoal = np.zeros(8760)\n",
    "\n",
    "print(southcoal[0])\n",
    "coalregs = {'NORTH': northcoal, 'NCENT': ncentcoal, 'EAST': eastcoal, 'COAST': coastcoal, 'SOUTH': southcoal, 'SCENT': scentcoal, 'WEST': westcoal, 'FWEST': fwestcoal}\n",
    "pd.DataFrame(data=coalregs).to_csv('CoalRegional2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import energy price data for each load region (different from weather regions)\n",
    "#we are converting SPP2019 spreadsheet to useable halfhourly energy price data for each load region\n",
    "#not using HB_HOUSTON (houston hub), COAST --> HB_SOUTH, NORTH --> HB_WEST\n",
    "y = 17520\n",
    "loadzones = ['HB_NORTH', 'HB_SOUTH', 'HB_WEST','HB_HUBAVG']\n",
    "loadprices = dict.fromkeys(loadzones,np.empty(0))\n",
    "\n",
    "epsheet = 'ERCOT/SPP2019.xlsx'\n",
    "months = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\n",
    "xls = pd.ExcelFile(epsheet)\n",
    "\n",
    "for i in range(12):\n",
    "    df = pd.read_excel(xls, months[i], usecols=['Settlement Point Name','Repeated Hour Flag','Settlement Point Price'])\n",
    "    df = df.loc[df['Repeated Hour Flag']=='N']\n",
    "    for zone in loadzones:\n",
    "        eprice = df.loc[df['Settlement Point Name']==zone]['Settlement Point Price'].to_numpy()\n",
    "        eprice = (eprice[::2] + eprice[1::2])/2 #15 min --> 30 min intervals\n",
    "        loadprices[zone] = np.hstack((loadprices[zone],eprice))\n",
    "\n",
    "#godam daylight savings time\n",
    "for zone in loadzones:\n",
    "    eprice = loadprices[zone]\n",
    "    #issue with daylight savings time in data\n",
    "    eprice = np.concatenate((eprice[0:3268],np.array([eprice[3267],eprice[3268]]),eprice[3268:17518]))\n",
    "    loadprices[zone] = eprice\n",
    "\n",
    "pd.DataFrame(data=loadprices).to_csv('ERCOT/EnergyPriceAgg2019.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
